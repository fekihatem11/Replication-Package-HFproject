{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731f3e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "556e48bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to retrieve question from StackOverflow using the StackExchange API\n",
    "def get_all_items(tag):\n",
    "    base_url = 'https://api.stackexchange.com/2.3/questions'\n",
    "    page = 1\n",
    "    has_more = True\n",
    "    all_items = []\n",
    "\n",
    "\n",
    "    while has_more:\n",
    "        params = {'page': page, 'pagesize': 100, 'order':'desc','sort':'activity','tagged':tag, 'site':'stackoverflow',\n",
    "                  'filter':'!67FLdRRZSxs2Fsn5eSX(_u4Pa)7QBCJmP-nCWIM1*fW2NjxHGzaul9a7c3m','key':'1w)qZgiYmE2CpM2JH1dfqA((' }  # Adjust 'pagesize' as needed\n",
    "        response = requests.get(base_url, params=params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            all_items.extend(data['items'])\n",
    "            has_more = data['has_more']\n",
    "            page += 1\n",
    "        else:\n",
    "            error_response = response.json()\n",
    "            print(f\"Request failed with status code {response.status_code} page number: {page} \" )\n",
    "            print(\"Error:\", error_response)\n",
    "            break\n",
    "\n",
    "    return all_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994172a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create a csv file from the returned JSON object\n",
    "def createCsv(csv_file_name):\n",
    "  # Get all unique keys from the JSON items\n",
    "  field_names = set().union(*(d.keys() for d in all_items))\n",
    "\n",
    "\n",
    "  # Writing to CSV\n",
    "  with open(csv_file_name, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "      writer = csv.DictWriter(csv_file, fieldnames=field_names)\n",
    "\n",
    "      # Write the headers\n",
    "      writer.writeheader()\n",
    "    \n",
    "      # Write each JSON item as a row in the CSV file\n",
    "      for item in all_items:\n",
    "          writer.writerow(item)\n",
    "\n",
    "  print(f\"CSV file '{csv_file_name}' has been created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caeaabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_items = get_all_items('huggingface')\n",
    "createCsv('huggingface.csv')\n",
    "all_items = get_all_items('huggingface-evaluate')\n",
    "createCsv('huggingface-evaluate.csv')\n",
    "all_items = get_all_items('huggingface-hub')\n",
    "createCsv('huggingface-hub.csv')\n",
    "all_items = get_all_items('huggingface-trainer')\n",
    "createCsv('huggingface-trainer.csv')\n",
    "all_items = get_all_items('huggingface-transformers')\n",
    "createCsv('huggingface-transformers.csv')\n",
    "all_items = get_all_items('huggingface-tokenizers')\n",
    "createCsv('huggingface-tokenizers.csv')\n",
    "all_items = get_all_items('huggingface-datasets')\n",
    "createCsv('huggingface-datasets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b5f8f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to combine all the files, each file belongs to a HF-related tag\n",
    "def combine_csv_files(file_list, key_column):\n",
    "    # Read and concatenate all CSV files\n",
    "    df_list = [pd.read_csv(file) for file in file_list]\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    # Drop duplicates based on the key column\n",
    "    combined_df.drop_duplicates(subset=[key_column], inplace=True)\n",
    "\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec8f726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we check for duplicates\n",
    "has_duplicates = concatenated_df.duplicated(subset='question_id').any()\n",
    "\n",
    "if has_duplicates:\n",
    "    print(\"Duplicates exist in the concatenated DataFrame.\")\n",
    "else:\n",
    "    print(\"No duplicates found in the concatenated DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c13d0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df.to_csv('HF_SO_2019_2024.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a7e805",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing (convert UnixTime to DateTime)\n",
    "\n",
    "df = pd.read_csv('HF_SO_2019_2024.csv')\n",
    "df.rename(columns={'creation_date': 'creation_date_unix'}, inplace=True)\n",
    "df.to_csv('HF_SO_2019_2024.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27475192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toDateFromUnix(column,dataset,fileName):\n",
    "  dataset[column] = pd.to_datetime(dataset[column], unit='s').dt.strftime('%Y-%m-%d')\n",
    "  dataset.to_csv(fileName, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6119e877",
   "metadata": {},
   "outputs": [],
   "source": [
    "toDateFromUnix('creation_date',dataset,'combined_huggingfaceTags_no_duplicates.csv')\n",
    "toDateFromUnix('last_activity_date',dataset,'combined_huggingfaceTags_no_duplicates.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
