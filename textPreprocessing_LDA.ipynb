{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7aa201",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import string\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "!pip install optuna\n",
    "import optuna\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(2018)\n",
    "\n",
    "\n",
    "!pip install pandas==1.5.3\n",
    "import pandas as pd\n",
    "import csv\n",
    "import logging\n",
    "import spacy\n",
    "import os\n",
    "\n",
    "!python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a8e2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        #removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "  ps = PorterStemmer()\n",
    "  return ps.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove code snippets enclosed in <code> tags\n",
    "    text = re.sub(r'<code>(.*?)</code>', '', text, flags=re.DOTALL)\n",
    "\n",
    "    # Remove HTML tags\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    text = soup.get_text(separator=' ')\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "    # Remove numbers and punctuation marks\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Remove English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join(word for word in text.split() if word.lower() not in stop_words)\n",
    "\n",
    "        # Add extra stopwords\n",
    "    extra_stop_words = set(['question', 'answer', 'etc','error','tri','answer','use',\"using\",\"trying\",\"try\"])  # Add more words as needed\n",
    "    stop_words.update(extra_stop_words)\n",
    "    text = ' '.join(word for word in text.split() if word.lower() not in stop_words)\n",
    "    \n",
    "    return text\n",
    "    \n",
    "\n",
    "def preprocess_lemmatize_stemming(text):\n",
    "    \n",
    "    result = []\n",
    "    for token in text:\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb1dac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "df= pd.read_csv(\"HF_SO_2019_2024.csv\")\n",
    "\n",
    "df['cleaned_title'] = df['title'].apply(preprocess_text)\n",
    "df['cleaned_body'] = df['body'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "df['corpus'] = df['cleaned_title'] + ' ' + df['cleaned_body']\n",
    "\n",
    "################\n",
    "data_words = list(sent_to_words(df['corpus']))\n",
    "    # Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=4, threshold=30) # higher threshold fewer phrases.\n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    #trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "    # Define functions for bigrams\n",
    "def make_bigrams(texts):\n",
    " return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "data_words_bigrams = make_bigrams(data_words)\n",
    "df['corpus']=data_words_bigrams\n",
    "#data_words_bigrams_df = pd.DataFrame({'Tokenized_Document': data_words_bigrams})\n",
    "#df_corpus.to_csv('corpus.csv', index=False)\n",
    "\n",
    "df['corpus'] = df['corpus'].apply(preprocess_lemmatize_stemming)\n",
    "\n",
    "words_to_transform = [\"huggingFace\", \"huggingfac\", \"hugging_face\",\"hugging_fac\",'hugging face']\n",
    "target_word = \"huggingface\"\n",
    "\n",
    "def transform_tokens(tokens, words_to_transform, target_word):\n",
    "    transformed_tokens = [target_word if token.lower() in words_to_transform else token for token in tokens]\n",
    "    return transformed_tokens\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "extra_stop_words = set(['follow','exampl','question', 'answer', 'etc','error','tri','answer','use',\"using\",\"trying\",\"try\",'huggingface'])  # Add more words as needed\n",
    "stop_words.update(extra_stop_words)\n",
    "\n",
    "\n",
    "def removeStopWords(tokens, words_to_remove):\n",
    "    return [token for token in tokens if token not in words_to_remove]\n",
    "\n",
    "df['corpus'] = df['corpus'].apply(lambda tokens: transform_tokens(tokens, words_to_transform, target_word))\n",
    "\n",
    "df['corpus'] = df['corpus'].apply(lambda tokens: removeStopWords(tokens, stop_words))\n",
    "\n",
    "df['corpus'].head(50)\n",
    "\n",
    "df.to_csv('HF_SO_2019_2024.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eac6ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "df = pd.read_csv('HF_SO_2019_2024.csv')\n",
    "df['corpus'] = df['corpus'].apply(lambda x: literal_eval(x) if isinstance(x, str) else x)\n",
    "# Import the wordcloud library\n",
    "from wordcloud import WordCloud\n",
    "# Join the different processed titles together.\n",
    "long_string = ','.join([' '.join(list_token) for list_token in df['corpus']])\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=10000000, contour_width=3, contour_color='steelblue')\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(long_string)\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745c2c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['corpus'].tolist()\n",
    "id2word = gensim.corpora.Dictionary(texts)\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "print([[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e662e135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence(model, corpus, corpora_dict):\n",
    "    coherence_model_lda = CoherenceModel(\n",
    "        model=model,\n",
    "        texts=corpus,\n",
    "        corpus=None,\n",
    "        dictionary=corpora_dict,\n",
    "        coherence=\"c_v\",\n",
    "    )\n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68e635e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def compute_intertopic_diversity_JSD(lda_model):\n",
    "    # Compute intertopic diversity using Jensen-Shannon divergence\n",
    "    topic_distributions = lda_model.get_topics()\n",
    "    intertopic_diversity = 0.0\n",
    "    num_topics = len(topic_distributions)\n",
    "    \n",
    "    for i in range(num_topics):\n",
    "        for j in range(i + 1, num_topics):\n",
    "            jsd = distance.jensenshannon(topic_distributions[i], topic_distributions[j])**2\n",
    "            intertopic_diversity += jsd\n",
    "    \n",
    "    intertopic_diversity /= (num_topics * (num_topics - 1)) / 2  # Average over all pairs\n",
    "    \n",
    "    return intertopic_diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebf99c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"final_result/model_results.csv\", \"a\") as f:\n",
    "        csvwriter = csv.DictWriter(\n",
    "            f, fieldnames=[\"trial\",\"coherence\",\"diversity\", \"ntopics\", \"alpha\", \"eta\",\"iterations\",\"chunksize\",\"passes\"]\n",
    "        )\n",
    "        csvwriter.writeheader()\n",
    "def write_model_results(model,trial ,coherence_score,diversity_score):\n",
    "    params = trial.params\n",
    "    trialnum = trial.number\n",
    "    with open(\"final_result/model_results.csv\", \"a\") as f:\n",
    "        csvwriter = csv.DictWriter(\n",
    "            f, fieldnames=[\"trail\", \"coherence\",\"diversity\", \"ntopics\", \"alpha\", \"eta\",\"iterations\",\"chunksize\",\"passes\"]\n",
    "        )\n",
    "\n",
    "        csvwriter.writerow(\n",
    "            {\n",
    "                \"trail\": trialnum,\n",
    "                \"coherence\": coherence_score,\n",
    "                \"diversity\": diversity_score,\n",
    "                \"ntopics\": params[\"num_topics\"],\n",
    "                \"alpha\": params[\"alpha\"],\n",
    "                \"eta\": params[\"eta\"],\n",
    "                \"iterations\": params[\"iterations\"],\n",
    "                \"chunksize\": params[\"chunksize\"],\n",
    "                \"passes\":  params[\"passes\"],\n",
    "\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    model_path = Path(f\"models/trial_{trialnum}\")\n",
    "    model_path.mkdir(parents=True, exist_ok=True)\n",
    "    model.save(str(model_path / f\"{trialnum}_lda\"))\n",
    "    top_words_filename = model_path / f\"trial{trialnum}_top_words.csv\"\n",
    "    get_and_save_top_words(model, top_words_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c0f9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_save_top_words(model, out_file):\n",
    "    top_words_per_topic = []\n",
    "    for t in range(model.num_topics):\n",
    "        top_words_per_topic.extend([(t,) + x for x in model.show_topic(t, topn=50)])\n",
    "    pd.DataFrame(top_words_per_topic, columns=[\"topic\", \"word\", \"p\"]).to_csv(out_file, index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6e34ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    alpha = trial.suggest_float('alpha', 0.01, 1, step=0.05)\n",
    "    eta = trial.suggest_float('eta', 0.01, 1,step=0.05)\n",
    "    ntopics = trial.suggest_int(\"num_topics\", 15,20)\n",
    "    iterations = trial.suggest_int(\"iterations\", 4000, 5000, step=10)\n",
    "    chunksize = trial.suggest_int(\"chunksize\", 20, 100, step=10)\n",
    "    passes = trial.suggest_int(\"passes\", 150, 300, step=10)\n",
    "    \n",
    "\n",
    "\n",
    "    model = gensim.models.LdaMulticore(\n",
    "\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        num_topics=ntopics,\n",
    "        random_state=300,\n",
    "        iterations=iterations,\n",
    "        chunksize=chunksize,\n",
    "        passes=passes,\n",
    "        alpha=alpha,\n",
    "        eta=eta,\n",
    "        per_word_topics=True,\n",
    "    )\n",
    "\n",
    "    #beta = model.get_topics()\n",
    "    #diversity_score= get_topic_diversity(beta, 10)\n",
    "    \n",
    "    diversity_score = compute_intertopic_diversity_JSD(model)\n",
    "    coherence_score = compute_coherence(model, texts, id2word)\n",
    "    print(f\"Trial {trial.number} coherence score: {round(coherence_score,3)} diversity score: {round(diversity_score,3)}\")\n",
    "    \n",
    "    write_model_results(model,trial, coherence_score ,diversity_score)\n",
    "\n",
    "    return coherence_score,diversity_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87a99f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def knee_point(p):\n",
    "\n",
    "# Calculate maximum coherence and diversity scores from Pareto front\n",
    "  coherence_scores_max = max([trial.values[0] for trial in p])\n",
    "  diversity_scores_max = max([trial.values[1] for trial in p])\n",
    "\n",
    "# Calculate the ideal point\n",
    "  ideal_point = (coherence_scores_max, diversity_scores_max)\n",
    "\n",
    "# Initialize variables for knee point\n",
    "  knee_point = None\n",
    "  min_distance = float('inf')\n",
    "\n",
    "# Find knee point using Euclidean distance\n",
    "  for solution in p:  \n",
    "        \n",
    "      coherence_score = solution.values[0]\n",
    "      diversity_score = solution.values[1]\n",
    "        \n",
    "      if diversity_score!=None:\n",
    "        \n",
    "        distance = math.sqrt((coherence_scores_max - coherence_score) ** 2 + (diversity_scores_max - diversity_score) ** 2)\n",
    "\n",
    "      # Update knee point if smaller distance found\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            knee_point = solution\n",
    "\n",
    "  return knee_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ae7171",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.multi_objective.create_study(\n",
    "    directions=[\"maximize\", \"maximize\"],\n",
    "    sampler=optuna.multi_objective.samplers.NSGAIIMultiObjectiveSampler()\n",
    ")\n",
    "\n",
    "result = study.optimize(objective, n_trials=500)\n",
    "best_trials = study.get_pareto_front_trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b81985",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparams = knee_point(best_trials)\n",
    "\n",
    "print(best_hyperparams.number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384afad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path =\"models/trial_\"+str(best_hyperparams.number)+\"/\"+str(best_hyperparams.number)+\"_lda\"\n",
    "loaded_model = gensim.models.LdaMulticore.load(model_path)\n",
    "lda_topics = loaded_model.show_topics(num_topics=-1, num_words=10, formatted=False)\n",
    "topics = loaded_model.print_topics(num_words=10)\n",
    "\n",
    "lda_topics_processed = [\n",
    "    [word for word, _ in words_with_probabilities]\n",
    "    for _, words_with_probabilities in lda_topics\n",
    "]\n",
    "\n",
    "# Displaying the processed topics\n",
    "for idx, topic_words in enumerate(lda_topics_processed):\n",
    "    print(f\"Topic {idx} - Words: {', '.join(topic_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6de1b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "diversity_score = compute_intertopic_diversity_JSD(loaded_model)\n",
    "coherence_score = compute_coherence(loaded_model, texts, id2word) \n",
    "print(diversity_score)\n",
    "print(coherence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadf8927",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = loaded_model.num_topics\n",
    "alpha = loaded_model.alpha\n",
    "eta = loaded_model.eta\n",
    "passes = loaded_model.passes\n",
    "iterations = loaded_model.iterations\n",
    "chunksiza= loaded_model.chunksize\n",
    "print(f'Number of topics: {num_topics}')\n",
    "print(f'Alpha: {alpha}')\n",
    "print(f'Eta: {eta}')\n",
    "print(f'Passes: {passes}')\n",
    "print(f'Iterations: {iterations}')\n",
    "print(f'chunksiza: {chunksiza}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b730467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = id2word\n",
    "topic_labels = {\n",
    "    0: \"Model customization\",\n",
    "    1: \"LLMs usage and understanding\",\n",
    "    2: \"Other\",\n",
    "    3: \"Model training\",\n",
    "    4: \"Other\",\n",
    "    5: \"Other\",\n",
    "    6: \"LLMs usage and understanding\",\n",
    "    7: \"Model Deployment\",\n",
    "    8: \"Other\",\n",
    "    9: \"Environment\",\n",
    "    10: \"Datasets\",\n",
    "    11: \"Model Loading, Saving, pushing\",\n",
    "    12: \"istributed Computing and Resource Management\",\n",
    "    13: \"LLMs usage and understanding\",\n",
    "    14: \"LLMs usage and understanding\",\n",
    "}\n",
    "\n",
    "from ast import literal_eval\n",
    "df = pd.read_csv('HF_SO_2019_2024.csv')\n",
    "df['corpus'] = df['corpus'].apply(lambda x: literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Function to assign topics to each post and get topic word distribution\n",
    "def assign_topics_to_posts(lda_model, post,topic_labels):\n",
    "    post_bow = dictionary.doc2bow(post)\n",
    "    topic_distribution = lda_model.get_document_topics(post_bow)\n",
    "    most_probable_topic = max(topic_distribution, key=lambda x: x[1])\n",
    "    topic_id, topic_prob = most_probable_topic\n",
    "\n",
    "    # Get the topic word distribution\n",
    "    topic_word_distribution = lda_model.get_topic_terms(topic_id)\n",
    "    terms = [dictionary.get(id) for id, _ in topic_word_distribution]\n",
    "    topic_label = topic_labels.get(topic_id, \"Unknown\")\n",
    "    \n",
    "    return topic_id, topic_prob, terms, topic_label\n",
    "\n",
    "# Apply the function to each row in the DataFrame\n",
    "df[['assigned_topic', 'topic_probability', 'topic_word_distribution', 'topic_label']] = \\\n",
    "    df['corpus'].apply(lambda post: pd.Series(assign_topics_to_posts(loaded_model, post, topic_labels)))\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "df.to_csv('HF_SO_2019_2024.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895cbd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_posts_for_each_topic(df, num_posts=50):\n",
    "    \n",
    "    # Initialize an empty DataFrame to store selected posts\n",
    "    selected_posts = []\n",
    "\n",
    "    # Get unique topics\n",
    "    unique_topics = df['topic_label'].unique()\n",
    "\n",
    "    # Iterate over each topic\n",
    "    for topic in unique_topics:\n",
    "        # Filter DataFrame for posts with the current topic\n",
    "        topic_posts = df[df['topic_label'] == topic]\n",
    "        topic_posts = topic_posts.sort_values(by='topic_probability', ascending=False)\n",
    "        selected_topic_posts = topic_posts.head(num_posts)\n",
    "\n",
    "        # Append selected posts to the selected_posts DataFrame\n",
    "        selected_posts.append(selected_topic_posts[['cleaned_title','cleaned_body', 'topic_label','topic_probability']])\n",
    "        selected_posts_df = pd.concat(selected_posts, ignore_index=True)\n",
    "    return selected_posts_df\n",
    "\n",
    "df = pd.read_csv('HF_SO_2019_2024.csv',on_bad_lines='skip')\n",
    "\n",
    "selected_posts = select_posts_for_each_topic(df, num_posts=50)\n",
    "\n",
    "selected_posts.to_csv('50_post_per_topic.csv', index=False)\n",
    "\n",
    "print(selected_posts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
